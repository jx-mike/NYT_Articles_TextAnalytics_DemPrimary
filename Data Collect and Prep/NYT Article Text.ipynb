{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get NYT Article Text\n",
    "\n",
    "The previous file gets all of the information for an article that is available via the NYT API, and downloads the html content to my local machine. This file opens each html document to get the actual article content, and will eventually push that information to MongoDB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All** text in **all** articles is in *p class=\"css-exrw3m evys1bk0\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put february article text into a list, where index in list is article_id\n",
    "feb_article_ids = list(range(0,6398,1))\n",
    "\n",
    "feb_article_text = []\n",
    "\n",
    "separator = ' '\n",
    "\n",
    "for article_id in feb_article_ids:\n",
    "    \n",
    "    with open('NYT_feb_articles/article'+str(article_id)+'.htm','r') as file:\n",
    "        article_text_list = []\n",
    "        soup = BeautifulSoup(file)\n",
    "        content = soup.find_all(\"p\", class_=\"css-exrw3m evys1bk0\")\n",
    "        for item in content:\n",
    "            pattern = re.compile(r'\\<.*?\\>')\n",
    "            text = pattern.sub('', str(item))    \n",
    "            article_text_list.append(text)\n",
    "        \n",
    "        article_text = separator.join(article_text_list)\n",
    "            \n",
    "        feb_article_text.append(article_text)\n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('feb_article_text.txt', 'w') as file:\n",
    "    for item in feb_article_text:\n",
    "        file.write('%s\\n' % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary where key = article_id and value = article text\n",
    "feb_dict = dict(zip(feb_article_ids, feb_article_text)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all articles with no mention of Bernie Sanders or Joe Biden\n",
    "# list of delete keys \n",
    "delete = [] \n",
    "for key, val in feb_dict.items(): \n",
    "    text = val.lower()\n",
    "    mr_sanders_presence = [m.start() for m in re.finditer('mr. sanders', text)]\n",
    "    senator_sanders_presence = [m.start() for m in re.finditer('senator sanders', text)]\n",
    "    bernie_presence = [m.start() for m in re.finditer('bernie', text)]\n",
    "    biden_presence = [m.start() for m in re.finditer('biden', text)]\n",
    "    \n",
    "    if ((len(mr_sanders_presence) == 0) & (len(senator_sanders_presence) == 0) & (len(bernie_presence) == 0) & (len(biden_presence) == 0)):\n",
    "        delete.append(key)\n",
    "    \n",
    "    #if val == 'Geeks': \n",
    "        #delete.append(key) \n",
    "          \n",
    "for i in delete: \n",
    "    del feb_dict[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "793"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many articles still present?\n",
    "len(feb_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('feb_dict.json', 'w') as file:\n",
    "    json.dump(feb_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put march article text into a list, where index in list is article_id\n",
    "march_article_ids = list(range(0,243,1))\n",
    "\n",
    "march_article_text = []\n",
    "\n",
    "separator = ' '\n",
    "\n",
    "for article_id in march_article_ids:\n",
    "    \n",
    "    with open('NYT_march_articles/article'+str(article_id)+'.htm','r') as file:\n",
    "        article_text_list = []\n",
    "        soup = BeautifulSoup(file)\n",
    "        content = soup.find_all(\"p\", class_=\"css-exrw3m evys1bk0\")\n",
    "        for item in content:\n",
    "            pattern = re.compile(r'\\<.*?\\>')\n",
    "            text = pattern.sub('', str(item))    \n",
    "            article_text_list.append(text)\n",
    "        \n",
    "        article_text = separator.join(article_text_list)\n",
    "            \n",
    "        march_article_text.append(article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('march_article_text.txt', 'w') as file:\n",
    "    for item in march_article_text:\n",
    "        file.write('%s\\n' % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary where key = article_id and value = article text\n",
    "march_dict = dict(zip(march_article_ids, march_article_text)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all articles with no mention of Bernie Sanders or Joe Biden\n",
    "# list of delete keys \n",
    "delete = [] \n",
    "for key, val in march_dict.items(): \n",
    "    text = val.lower()\n",
    "    mr_sanders_presence = [m.start() for m in re.finditer('mr. sanders', text)]\n",
    "    senator_sanders_presence = [m.start() for m in re.finditer('senator sanders', text)]\n",
    "    bernie_presence = [m.start() for m in re.finditer('bernie', text)]\n",
    "    biden_presence = [m.start() for m in re.finditer('biden', text)]\n",
    "    \n",
    "    if ((len(mr_sanders_presence) == 0) & (len(senator_sanders_presence) == 0) & (len(bernie_presence) == 0) & (len(biden_presence) == 0)):\n",
    "        delete.append(key)\n",
    "    \n",
    "    #if val == 'Geeks': \n",
    "        #delete.append(key) \n",
    "          \n",
    "for i in delete: \n",
    "    del march_dict[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many articles still present?\n",
    "len(march_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('march_dict.json', 'w') as file:\n",
    "    json.dump(march_dict, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
